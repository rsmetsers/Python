\section{Method}\label{sec:method}



%In an explorative small-scale case study we investigated our pedagogical approach.

The qualitative research will be implemented in several studies over four phases, relating to the four subquestions.




\subsection{Study I. Relevant assessment design principles.}
\textit{What are the relevant assessment design principles?}\newline
Literature research will be used to answer the following questions:

\begin{enumerate}
\item What are general assessment characteristics and quality criteria?
\item Which criteria and relevant aspects do current CS assessment tools adhere to?
\item Which validated processes can be used to come to qualitative assessments?
\item Which taxonomies and frameworks are used in CS assessment?
\end{enumerate}

Algorithmic Thinking is one of CT's fundamental aspects\ref{CTdefWing}. While many recent efforts have focussed on definition issues of CT (Grover and Pea, 2013), work specifically pertaining to Algorithmic Thinking remains sparse. As Algorithmic Thinking is a prominent component of CT, we incorporated general CT research in our review.



\subsection{Study II. Eliciting and judging evidence of relevant knowledge and skills.}
\textit{How to elicit and judge evidence of relevant knowledge and skills?}\newline

In order to systematically design assessment tasks, we apply the Evidence Centered Assessment Design (ECD) framework.

 Borrowing from the Evidence-Centered Design (ECD) Assessment Framework. It includes the iterative process of developing, piloting, testing and evaluating an assessment design pattern. The ECD process ensures coherence between learning objectives and tasks which elicit evidence in order to adequately assess these. The output is an assessment design pattern (consisting of templates and example assessments). The following questions will be answered:

\begin{enumerate}
\item What are the learning objectives and Focal KSAs?
\item Which taxonomies and frameworks are used in CS assessment to indicate achievement levels?
\item What are the potential observations with which students can show-off the proficiency of their skills and knowledge?
\item What are the potential work products? Which tools or instruments work as enables, placing students in a stimulating situation to excel and demonstrate their capabilities.
\item What are the task variables? (i.e., which features can be modified to create new tasks or to increase or decrease difficulty?)
\item How to score? Guidelines regarding the test specification and scoring procedures related to each specific work product.
\end{enumerate}

Two related studies report positive results from such a structured approach, Snow et. al. (2017) for the ECS framework and Catete (2017) for AP CS Principles. Both the CS Advanced Placement and ECS exams have been validated (2010TewGuzdial). This research follows a similar approach. 


As a starting point, relevant international curricula were reviewed. To select the curricula, we first listed those curricula which have been referred to in recent publications. We then selected those curricula who’s curricular learning objectives coincided with those in the Netherlands with a particular focus on algorithms or problem solving, as well as explicitly embedding Computational Thinking as a core competency. In the third selection, relevant supportive documentation was considered, such as progression pathways (CAS, corresponding to the national curriculum in the U.K.) or specific description of the KSA outcomes and mappings to their corresponding curricula (ECS and AP CS Principles as specifications of the U.S. CSTA, and AQA as specifications of the England national curriculum). This narrowed down to the following short-list of curricula: U.S. CSTA and National curriculum for computing England. From these curricula, sample assessment tasks and the way in which they elicited evidence in relation to their KSAs was analyzed.

\textbf{Eliciting PCK evidence}\newline
To determine how to effectively support teachers with their assessments we let teachers create their own assessments using the template and model task and use interviews and observations to investigate the relationship with PCK. In addition, we analyze the assessment they create across the quality characteristics that were predetermined in the initial phase of the study.

A participatory design was used in which teachers were involved in, and played an active role in creating assessments. Teachers were given the design pattern and accompanying example task and were asked to create a personalized assessment of their own. They were asked to jot down the process, how they approached it, how much time they spent, if they encountered any problems, what they did to overcome these hurdles, and any suggestions for improvement. They were allowed to ask for support. If this was the case this was noted.


After assessment delivery, the teachers were asked about their approach and experience using the design template via a semi-structured interview. The fit-for-purpose and fit-for-use topics considered came from 'The Wheel of Competency Assessment'. In addition they were asked about suggestions for improvements. The teachers were asked to convey the notes they had made during the implementation step.


\textbf{Establishing quality of assessment}\newline


For validation purposes, think-aloud sessions were held with the students. The goal was to ensure that the intended objectives were being assessed, and at the intended level.

As a part of measuring the quality, mapping to the relevant KSAs was performed, as well as noting the use of variable features of tasks (difficulty, contexts), and any other particular adjustments made or characteristics added. To evaluate objectivity of the teacher's assessment, the scores were compared to those given by the teacher.

\textbf{Expert panel}\newline
Intermediary results will be prone to expert review, by a carefully selected panel of experts. The group consists of educators, practitioners and experts from multiple institutions (two high schools and two universities), as well as experts that have been involved in conceiving the reformed Dutch curriculum.

Different methods can be used to come to a consensus. The Delphi method is rather cost-effective. Furthermore, as the number of experts in the field can be counted on one hand, the anonymity cannot be guaranteed. Rather, similar to Catete (2017) we choose to use a mix between the Nominal Group Technique and the Delphi technique. First, every member of the group responds to the given input, explaining their outcomes shortly. Any items for which consensus has been found is removed, in essence, creating a short-list of controversy items. The members are then again asked, now that they have heard the reasoning of their colleagues, to reconsider their view. This is repeated until a minimal consensus of 70\% is reached.

The panel will contribute in the following stages:
\begin{itemize}
\item To ensure that the resulting KSAs do indeed match the learning objectives, are complete, and adhere to a classroom situation.
\item To validate the model assessment that created using the design pattern.
\item To review if exemplar tasks are correctly categorized and indeed assess the specific KSA as intended.
\end{itemize}




\subsection{Study III. Bridging the gap of assessment knowledge.}
\textit{How to effectively bridge teachers' assessment knowledge (as part of PCK) gap?}

\begin{enumerate}
\item What do we currently know about teachers' assessment PCK?
\item Which PCK aspects are needed for the assessment of algorithms? (i.e., which content knowledge, w.r.t. observing evidence, knowledge about what is challenging for students)
\item Which (characteristics of) tools or instruments enable teachers to adequately observe and score the level of achievement of the learning objective, and thus support teachers with summative assessment?
\end{enumerate}

This study starts with a literary review to inventorize what is known about teachers' PCK pertaining to assessment in general, and that of algorithms in particular. The results from literary review will be triangulated with teacher interviews and observations during the implementation phase in which teachers use templates to create and employ their customized assessments.

\todo{evt rubrics etc}

The study continues with a document analysis of existing tools or instruments from related work. CS AP Principles (Snow et. al., 2017) and ECS (Catete, 2017) have used a structured approach to establishing assessments, resulting in a clear link between potential observations (evidence of students' knowledge or skills), evaluation procedures, and measurement models. The results of this analysis are used as input for the conceptual assessment framework level (of the ECD process) in the development of the design template. In addition, relevant (inquiry based) tasks from Process Oriented Guided Inquiry Learning in Computer Science (CS-POGIL), the International Baccalaureate (IB) program, and the Computer Science Field Guide (\url{http://csfieldguide.org.nz/}).



Following the ECD process further, the design pattern is piloted in two stages. Via qualitative analysis (teacher interviews, student think-aloud sessions, assessment task analysis, scoring process analysis) we seek to determine which characteristics in the design-patterns are feasible as PCK-enablers, and which aspects lack in quality. In an iterative process, the results are analyzed and evaluated, and the design pattern adjusted an refined. We subsequently report our findings. This process is repeated twice, first in a small-scale pilot, and then the candidate template assessment tasks will be deployed at multiple institutions for full-scale testing.


\subsection{Study IV. Variable features: untangling programming tasks}
\textit{How can the variable features of task difficulty be identified and accounted for and integrated in an assessment?}

Input for this study is the result from the outer three layers of ECD (study II).

Our research builds on the work of Luxton-Reilley (2018) in which they analyse assessments for introductory programming courses (CS1), identify relationships and progression between concepts and decompose assessments into elements that can be assessed independently. Our research extends their study by focussing on secondary education (as opposed to CS1) and by including plan composition strategies. De Raadt et al. (2009) distinguished a set of elementary plans (programming strategies) that can be abutted, nested and merged to create more complex plans.


\todo{Plans give means to reasoning on a higher level of abstraction. By decomposing complex problems (or solutions) and recognizing relevant plans and incorporating them into a solution, ... facilitates evaluation.}


\todo{The use of plans assists the abstraction and decomposition of open-ended problems
Algorithms can be viewed as aspects of Computational Thinking (CT) as it involves abstraction and decomposition of open-ended problems, the construction of a solution via algorithmic thinking and generalization and the evaluation of the provided result (Wing, 2006)}

 Strategies can be expressed both simply, at a subalgorithmic level, and at a higher level. These levels coincide with the SOLO categories, as described by Lister et. al. (2010) and Meerbaum-Salant et. al. (2010, 2013). This abutment, nesting and merging of plans encompass algorithmic thinking and problem solving aspects which should be taught and assessed explicitly (de Raadt, 2009).




Similar to the work of Luxton-Reilley (2018), qualitative Content Analysis was performed to assign conceptual categories to tasks.


\todo{Maar wij gaan verder, want zij hebben}


This entailed a combination of a top-down and a bottom-up approach. In a top-down deductive fashion (Friese, 2018), relevant concepts were distilled from the Luxton-Reilly paper (2018), plans derived from de Raadt's work (2008), and the KSAs which followed from the curricular learning objectives (Study II). Grounded Theory (Corbin and Strauss, 1998) was used as a bottom-up approach. Inductive content analysis ensured that the concepts represented in the KSA were complete. In an explorative process, educational materials and assessment repositories where analyzed in a more inductive coding process to determine any additional concepts or plans. Input was primarily past exam papers (AP CS Principles, ECS, IB and AQA AS/A). These examples were augmented by examples discussed and analyzed in well-cited related work, such as Fisler's (2014) Rainfall Problem. Other resources include Whalley (2006), Meerbaum-Salant et al. (2010, 2013), and Luxton-Reilly (2018), Lister (2016), Simon (2016), and Teague (2014). Relevancy was determined by checking if it matched to one of the established KSAs.% accommodated for more tasks. Examples included  Atmatzidou en Demetriadis (2016), Liu et al. (2013), , Thompson (2016), Lopez (2008),  Zur-Bargury (2013).

Only comprehension tasks were considered (as opposed to solution creation tasks). For each topicParticular attention was paid to include documented student misconceptions (such as described on \url{PD4CS.org}, Grover(2017), Sorva (2012)), as it is of importance to identify these as soon as possible and help students overcome corresponding challenges.

\todo{ Specifieker:bijvoorbeeld voor elk leerdoel uit elk repository 10 assessments paken.}
\todo{ Hoe… bij elk leerdoel meerdere alternatieven ??? .. ook even misconceptions noemen als extra tasks }

Whilst most research and thus task examples are derived from CS1 university courses, expert review was used to ensure that the tasks are applicable to secondary education. The expert evaluation also revealed whether the tasks were correctly categorized and indeed assess the specific KFSA as intended.

\todo{NU}

\todo{Ook weer dubbelop?}
The content analysis took place in three phases. In the pre-analysis phase we established a list of codes. The codes were derived from the relevant KSAs emerging from the Domain Modelling stage and entail important concepts and plans. In the second phase material exploration took place. The documents imported into Atlas and were coded in a top-down deductive fashion (Friese, 2018). In the third phase, the data was analyzed and interpreted. A last step was an inductive content analysis to detect any omitted concepts or plans. Such a discovery is a possibly signal of an omission in the KSA, and was presented to the expert panel for review.


%As peer-reviewed assessment repositories for secondary education are sparse, repositories for CS1 were included and the tasks individually reviewed for coincidence with the established KSAs. The following repositories were short-listed: Project Quantum (a CAS initiative) and Canterbury Question Bank (CS1 and CS2).


Candidate assessment tasks were selected and coded (in Atlas) according to the established list of concepts and plans. Topics not relevant to the KSAs were omitted (for example Object-Oriented Concepts). In addition, only comprehension tasks (code reading) were considered (as opposed to solution creation tasks).



\todo{samentrekken met twee paragraphs higher????}
In our analysis, we take this plan-integration concept a step further. We also consider the bottom-up integration of individual concepts, based on the knowledge concepts identified during Domain Modelling stage. A top-down approach will be used to ensure that the resulting tasks are meaningful, i.e. can be considered a (partial) solution to some problem. Some integrated concepts are undesired and should thus be omitted as a task (for example, printing a variable and then declaring it does not result in valid code.)


Using the coded work in atlas, the tasks will be analyzed to establish a hierarchy of precedence. As such, we can establish which concepts are prerequisite to which type of algorithmic composition plans. For example, a task including a selection statement with a compound boolean expression:
\begin{verbatim}
if (x>3 and x<5):
    print B

print E
\end{verbatim}
requires knowledge about how a (one-branch) selection statement works, arithmetic operators (\jav{<} and \jav{>}), and the boolean operator (\jav{and}). A student who fails to comprehend the question adequately could be dealing with a misconception of any one of these three concepts. Traditionally, one may attempt to establish a multiple-choice question with distracters to pinpoint the misconception, this has a few drawbacks. Firstly, it is very difficult to establish meaningful distracters, contrarily, students are sometimes even be provoked or put on the wrong track by distracters. Furthermore, if a student is struggling with two of these concepts, only one will be brought to light. In order to determine what the particular misconception is that the student is struggling with, required answering many more tasks. As assessments are created a priori, the tasks are not relevant to, or correspond with, the students level of understanding. Pre-defined tasks may be more complex, such as the previous question extended into a multi-branch selection. With the knowledge that the student could not answer the previous question, such an extended question is not reasonable to ask. It does not elicit any extra knowledge about the students level of attainment, and thus only bourdons the student's time and possibly lowering their self-efficacy. In turn, this last aspect is not constructive for the motivation (one of the characteristics which good assessments should adhere to, see Study I). Contrarily, if a student fails to complete this task correctly, they should be given 3 subtasks, one for each of the concepts which it entails. Obviously, the student must complete 3 extra questions. But it does allow for a more precise assessment, pointing out exactly which concept the student is struggling with and what the student is proficient in. Such feedback could well be used for both formative (feedback on what to work on) and summative assessment.


 This output of this effort contributed as input for the 'task variables' of the design template (part of the ECD Conceptual Assessment Framework level, study II). Subsequently, a pilot was initiated to research the design template from the Assessment Implementation and Assessment Delivery layers. During the implementation stage, the model assessment tasks were translated into classroom-specific assessment tasks in the designated programming language. The task variables can be integrated to vary the difficulty of a task.



In the Assessment Delivery stage, the tasks were administered to students.



Brennan and Resnick (2012) identify the distinction between strategic knowledge (Computational Practice) and structure/plan knowledge (Computational Concepts). Strategic knowledge, such as algorithmic plan composition strategies can rarely be deduced by merely inspecting a given answer. Thus, to give insight on how the student tackles the problem at hand, and evaluate if this was on the intended cognitive level, evidence must be collected in a different manner.  Ericsson and Simon (1993) used a think-aloud protocol so that students can verbalize their cognitive processes. In their research into CT skills Atmatzidou and Demetriadis (2015) employ the same method to allow students to express their thinking more freely. Along the same line, Whalley et. al. (2006) and Snow et. al. (2017) mimicked Lister et. al.'s (2004) think aloud protocol sessions. After having done so, they conclude that this is the best way to determine the corresponding taxonomy level of a task. We held and audio recorded think-aloud sessions with students whilst they were completing the tasks. As proposed by Tew and Guzdial (2010), students' scores from the pilot are evaluated and compared to those attained during the traditional course assessment as an indication of reliability. After the pilot was performed, the candidate template assessment tasks were deployed at multiple institutions for testing. After each iteration, the results will be analyzed and evaluated again, and the instrument refined.

After the pilot was performed, the  candidate template assessment tasks were deployed at multiple institutions for testing on a larger scale. The results were analyzed and evaluated again, and the instrument refined.

