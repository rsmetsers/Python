\section{Method}\label{sec:method}



%In an explorative small-scale case study we investigated our pedagogical approach.

The qualitative research will be implemented over two (partially intertwined) strands, through which the four subquestions will be considered:
\begin{enumerate}
\item The first strand pertains to investigating ways in which teachers can establish qualitative assessment tasks for the new Dutch curriculum.
\item The second is called "untangling programming tasks" and investigates a framework for decomposing and arranging programming (sub)tasks in a manner to specifically identify misconceptions in student learning, and variable features in programming tasks.
\end{enumerate}




\subsection{Subquestion I. What are the relevant assessment design principles?}%Relevant assessment design principles.
Literature research will be used to answer the following questions:

\begin{enumerate}
\item What are general assessment characteristics and quality criteria?
\item Which criteria and relevant aspects do current CS assessment tools adhere to?
\item Which validated processes can be used to come to qualitative assessments?
\item Which taxonomies and frameworks are used in CS assessment?
\end{enumerate}

Algorithmic Thinking is one of CT's fundamental aspects (see section \ref{CTdefWing}). While many recent efforts have focussed on definition issues of CT \cite{GroverPea2013}, work specifically pertaining to algorithmic thinking remains sparse. As algorithmic thinking is a prominent component of CT, and there is more research on CT than on algorithmic thinking, we incorporate general CT research in our review.



\subsection{Subquestion II. How to elicit and judge evidence of relevant knowledge and skills?}%Eliciting and judging evidence of relevant knowledge and skills.

%OOK PLANS==PRACTICES
In order for a teacher to judge a student's knowledge and skills (practices), a student must be placed in a situation or given a task that will elicit the desired evidence. Which evidence is relevant can be inferred from the high-level curricular goals. The design of high-quality assessment materials ensures a correct mapping from those curricular goals to a design pattern consisting of three aligned models (student model, evidence model, and task model) which together form the assessment specifications: the foundation for the new assessment.

This research borrows aspects from two structured approaches which have both been implemented in secondary CS education and its implication researched and reported on. In order to systematically design assessment tasks, we apply the Evidence Centered Assessment Design (ECD, see section \ref{sec:ECD}) framework combined with validation through an expert panel (see \cite{catete2017framework}). Data about the resulting assessment (last bullet below) is elicited through semi-structured teacher and (a selection of strong, medium and weak) student interviews, think-aloud sessions of student self-evaluation, and a second-opinion re-evaluation of scoring of (a selection) of students' work.

While traversing ECD's five levels, the following questions will be answered:

\begin{enumerate}
\item What are the learning objectives and Focal KSAs?
\item Which taxonomies and frameworks are used in CS assessment to indicate achievement levels?
\item What are the potential observations with which students can show-off the proficiency of their skills and knowledge?
\item What are the potential work products? Which tools or instruments work as enables, placing students in a stimulating situation to excel and demonstrate their capabilities.
\item What are the task variables? (i.e., which features can be modified to create new tasks or to increase or decrease difficulty?)
\item How to score? Guidelines regarding the test specification and scoring procedures related to each specific work product.
\item Which quality criteria do assessments created by teachers, based on a design template, adhere to?

\end{enumerate}





\subsection{Subquestion III. How to effectively bridge teachers' assessment knowledge (as part of PCK) gap?}%Bridging the gap of assessment knowledge.


As described in the background section, teachers face many challenges while creating assessment tasks. They could benefit from assessment templates which they can fine-tune according to their own specific classroom needs. This could help them establish qualitative assessments which adhere to the curricular objectives. We investigate this conjecture, and its significance and implications.

Specifically, we answer the following questions:
\begin{enumerate}
\item What do we currently know about the challenges teachers face pertaining to assessment (as a part of PCK)?
\item Which PCK aspects are important for the assessment of algorithms? (i.e., which content knowledge, w.r.t. observing and evaluating evidence, what type of knowledge about which content or skills is challenging for students?)
\item Which (characteristics of) tools or instruments enable teachers to adequately observe and score the level of achievement of the learning objective, and thus support teachers with summative assessment?
\item Does fine-tuning an assessment template support teachers in creating qualitative assessment tasks? (i.e., Which challenges do teachers face whilst fine-tuning an assessment template? Which, if any, hurdles does it help overcome? Which aspects of the template were (not) used and why?)
\end{enumerate}



\noindent\textbf{Eliciting PCK evidence}\newline
To determine how to effectively support teachers with their assessments we let teachers create their own assessments based on template and model tasks from the ECD process. Thus, a participatory design is used involving teachers who play an active role in creating assessments. After a short instruction, teachers are given the design pattern and accompanying example tasks and are asked to create, implement and deliver a personalized assessment of their own.

After assessment delivery, the teachers were asked about:
\begin{itemize}
\item experience using the design template (i.e., how much time they spent, if they encountered any problems, if they felt it was useful)
\item how they approached the task (i.e., how they started, what they did to overcome any hurdles),
\item the quality criteria aspects discussed in section \ref{sec:qualityCriteria},
\item any suggestions for improvements.
\end{itemize}
To investigate the relationship with PCK, we use semi-interviews, observations, and teachers' notes on their thoughts and experiences during implementation. Teachers were allowed to ask for intermediate support. If this was the case, it was noted.


\noindent\textbf{Establishing quality of assessment}\newline

To evaluate the quality of the design template, it (or its resulting teachers' assessment) is aligned across the quality criteria described in section \ref{sec:qualityCriteria}. Table \ref{table:MappingCriteriaMethod} shows how the different methodological aspects are used to evaluate the quality criteria of the assessment design template.

%As a part of measuring the quality, mapping to the relevant KSAs was performed, as well as noting the use of variable features of tasks (difficulty, contexts), and any other particular adjustments made or characteristics added.


%A summary of the processes for meeting Baartman's Wheel of Competency Criteria\todo{aanpassen}


\begin{table*}
  \centering
\begin{tabular}{|c|p{20mm}|p{33mm}|p{33mm}|p{33mm}|}
  \hline
   & \textbf{Criteria} & \textbf{Evidence Type} & \textbf{Participants} & \textbf{Success criteria}\\
  \hline

  1& Fitness for purpose & ECD process and Expert review &  Master teachers, teacher educators, curricular experts & Rubrics consider CT and CS concepts and skills for specific assessment\\ \hline
  2& Comparability & Evaluation Rubrics & Teachers & Structure of rubric provides consistent criteria \\ \hline
  3&Acceptability & Semi-structured interview \& Expert review & Teachers, Master teachers, teacher educators, curricular experts & Used by multiple teachers and expert panel consensus\\ \hline
  4&Transparency & Semi-structured interviews & Teachers, (selection of) students  & Consistent scores between student self-evaluation and teacher evaluation\\ \hline
  5&Reproducibility of decisions & Independent re-evaluation of students' work (random selection) & Teachers & Consistent results by teacher and during re-evaluation\\  \hline
  6&Authenticity & Expert review & Master teachers, teacher educators, curricular experts  & Expert panel consensus \\ \hline
  7&Fairness & Semi-structured interviews \& student self-evaluation \& Expert review  & Teachers \&  students, Master teachers, teacher educators, curricular experts  & Teacher and self-assessment are in line, expert panel consensus\\ \hline
  8&Cognitive Complexity & Expert review & Master teachers, teacher educators, curricular experts & Assignment and rubrics include relevant\footnote{\emph{Relevant} here means both pertaining to the assignment at hand, as well as corresponding to skills needed in the future.} CT skills \\ \hline
  9&Meaningfulness & Semi-structured interviews & Teachers & Rubrics yield meaningful feedback, teachers feel confident assessing student learning \\ \hline
  10&Fitness for Self-Assessment & student self-assessment & Students & students capable of adequately assessing own work \\ \hline
  11&Educational Consequences & Semi-structured interviews, self-evaluation& Teachers, students & Identification of students' misconceptions and weaknesses
  \\ \hline
  12&Costs \& Efficiencies & Semi-structured interviews & Teachers & Teachers indicate a reduction in time and costs for assessment implementation and delivery\\
  \hline
\end{tabular}
\caption{Methods for evaluating quality criteria}\label{table:MappingCriteriaMethod}
\end{table*}


\todo{BIJ RESULTS: For each of the results, see if it pertains to the template itself, or is specific to the created assessment template.}

In some cases, conjecture about the quality will be made based on the design template, in others on the fine-tuned assessment implemented by the teacher (for example with data from student interviews or think-aloud sessions). The origin of the data will be considered during evaluation of the results.


\subsection{Subquestion IV. How can the variable features of task difficulty be identified, accounted for and integrated into an assessment?}%Variable features: untangling programming tasks}


Our research builds on the work of \citeA{LuxtonReilly2018} in which they analyze assessments for introductory programming courses (CS1), identify relationships and progression between concepts and decompose assessments into elements that can be assessed independently. We extend their work with two aspects:
\begin{enumerate}
\item Including \emph{plans}: \citeA{deRaadt2009teachingPlans} distinguishes a set of elementary plans (programming strategies) that can be abutted, nested and merged to create more complex plans. These plan composition strategies encompass algorithmic thinking and problem solving aspects which students find challenging. According to \citeauthor{deRaadt2009teachingPlans}, these should be taught and assessed explicitly.


\item Focus on \emph{secondary education}: We focus on CS and CT concepts that are of primary interest for secondary education (as opposed to CS1, which most current research is based on).

\end{enumerate}

\todo{hier mag nog wel wat aan gedaan worden! Dat Transfer moeilijk is moet ook in de background}
In addition, this question includes analyzing variable features as the 'recontextualization' of a given algorithmic problem and the transfer of knowledge which accompanies it. The question is thus how to best assess specific knowledge and practices pertaining to a new context? The challenge here is to assess the algorithmic concepts and practices, rather than the capability to transfer knowledge.


\subsection{Strand I. Method: untangling assessment tasks}

This study can be split into several phases:
\begin{enumerate}
\item Identifying the relevant concepts and plans that are specific to secondary education.
\item Identifying the relationships between concepts and plans, and incorporate plans into a coherent progression pathway.
\item Analyzing current research and exams on the complexity of their assessment tasks and inclusion of assessment of plans.
\item Creating assessment tasks that pinpoint achievement levels which differentiate between strategy and knowledge.
\item Implementing and delivering a pilot assessment in a real secondary education setting.
\item Analysing the impact of implementing the assessment tasks on both students and teachers as well determining the conditions and teachers' PCK needed to do so successfully.
\end{enumerate}



\noindent \textbf{Identifying relevant concepts and plans, and their relationships}\newline
The goal is to identify which conceptual knowledge and which conceptual practices (including strategies) are relevant in a secondary education setting. This will result in a hierarchy, or progression pathway, which indicates the relationship between concepts and plans.

We entail a combination of a top-down and a bottom-up approach.

The content analysis took place in three phases:
\begin{enumerate}
\item In the pre-analysis phase we established a list of codes. The codes are derived from the results of the Domain Modelling stage layer of ECD, which entail the CS and CT concepts and plans that are of primary interest for secondary education.

\item In the second phase material exploration took place. In a top-down deductive fashion (Friese, 2018), relevant concepts are distilled from the \citeauthor{LuxtonReilly2018}'s paper, plans derived from \citeauthor{deRaadt2009teachingPlans}'s work, and the KSAs which follow from the curricular learning objectives. Similar to the work of \citeauthor{LuxtonReilly2018}, qualitative Content Analysis is performed to assign conceptual categories to tasks. The documents imported into Atlas and coded.

\item In the third phase, the data was analyzed and interpreted. Grounded Theory (Corbin and Strauss, 1998) is used as a bottom-up approach. Through inductive content analysis, any omitted concepts or plans are to be detected. Such a discovery is a possibly signal of an omission in the KSA, and will be presented to the expert panel for review. Input is primarily past exam papers (AP CS Principles, ECS, IB, Israel's 2012 CS exam [Zur-Bargury (2013)].


\end{enumerate}


\noindent \textbf{Creating assessment tasks in line with progression pathway}\newline
%that pinpoint achievement levels which differentiate between strategy and knowledge
The goal of this study is to come to a set of template tasks which assess concepts and/or plans individually, in addition to meaningful algorithmic (plan-composition) combinations.
\todo{is dit zo. kun je niet met parsons en flowcharts composition testen}
In a first stage, only comprehension tasks are considered.% at this point in time. % (solution creation tasks (writing code) is beyond the scope of this research).
In a second stage, we determine feasible manners to assess plan strategies, with or without having to compose code. %OF HET KAN MET EEN PO, TOCH?



An expert panel will review whether the template tasks are correctly categorized and indeed assess the specific KSAs as intended, and thus to ensure that the tasks are applicable to secondary education. For each KSA, a minimal of one template task is to be established. As it is of importance to identify student misconceptions as soon as possible and help students overcome corresponding challenges, for each topic (concept or plan), particular attention is paid to include these (such as documented on \url{PD4CS.org}, \cite{grover2017measuring}, \todo{cite}Sorva (2012)).

Using the coded work in atlas, the tasks will be analyzed to establish a hierarchy of precedence. As such, we can establish which concepts are prerequisite to which type of algorithmic composition plans. For example, a task including a selection statement with a compound boolean expression:
\begin{verbatim}
if (x>3 and x<5):
    print B

print E
\end{verbatim}
requires knowledge about how a (one-branch) selection statement works, arithmetic operators (\jav{<} and \jav{>}), and the boolean operator (\jav{and}). A student who fails to comprehend the question adequately could be dealing with a misconception of any one of these three concepts.



Thus, in our analysis, we take \citeauthor{LuxtonReilly2018}'s plan-integration concept a step further. We also consider the bottom-up integration of individual concepts, based on the knowledge concepts identified during Domain Modelling stage. A top-down approach will be used to ensure that the resulting tasks are meaningful, i.e. can be considered a (partial) solution to some problem. Some integrated concepts are undesired and should thus be omitted as a task (for example, printing a variable and then declaring it does not result in valid code.)


The result is a progression pathway with accompanying assessment tasks for each level. The progression pathway will be established iteratively, as results from the pilot phase will be used as input to adjust where necessary.


\noindent \textbf{Analyzing current research and exams on the complexity of their assessment tasks}\newline
Well-cited related work will be analyzed to determine if indeed, as suggested in \citeA{Whalley2006}, multiple concepts are generally simultaneously assessed. This articles include Fisler's (2014) Rainfall Problem, Whalley (2006), Meerbaum-Salant et al. (2010, 2013), Luxton-Reilly (2018), Lister (2016), Simon (2016), Teague (2014), Zur-Bargury (2013).

As most research is focussed on CS1, the analysis includes mapping to secondary education curricula and pathway and identifying deficiencies. The results could be used as a benchmark to compare the new assessment tasks to traditional tasks.

%Relevancy is determined by cross-checking whether there is a link to one or more of the formulated KSAs.\todo{MISSCHIEN TOEVOEGEN?? accommodated for more tasks. Examples included  Atmatzidou en Demetriadis (2016), Liu et al. (2013), , Thompson (2016), Lopez (2008),  Zur-Bargury (2013).}
%


\noindent \textbf{Implement and deliver a pilot assessment}\newline

The template tasks will be translated to concrete classroom-specific assessment tasks in the designated programming language, and administered to a selection of 5 students in a pilot. A student will be given an assessment task to complete. The subsequent tasks will be determined on the basis of the hierarchy.

Think-aloud sessions will be held with the students (and audio-recorded) to reveal if the task does indeed assess the correct concept and achievement level as intended. As proposed by \citeA{2010TewGuzdial}, as an indication of reliability, students' results from the pilot are evaluated and compared to those attained through formative assessments throughout the rest of the course.




%As peer-reviewed assessment repositories for secondary education are sparse, repositories for CS1 were included and the tasks individually reviewed for coincidence with the established KSAs. The following repositories were short-listed: Project Quantum (a CAS initiative) and Canterbury Question Bank (CS1 and CS2).

%
%Candidate assessment tasks were selected and coded (in Atlas) according to the established list of concepts and plans. Topics not relevant to the KSAs were omitted (for example Object-Oriented Concepts). In addition, only comprehension tasks (code reading) were considered (as opposed to solution creation tasks).

The impact of implementing the assessment tasks on both students and teachers will be analyzed. The goal is to determine relevant conditions and teachers' PCK needed for successful implementation and delivery.


\subsection{Strand II. Method: assessment design template}
Two related studies report positive results from a (semi-)structured approach to establishing assessments, \citeA{mislevy2006implications} and \citeA{snow2017CTECD} for the ECS framework and \citeA{catete2017framework} for Advanced Placement CS Principles. Both the AP CS Principles and the ECS exams have been validated \cite{2010TewGuzdial}. This research borrows aspects from both approaches. In order to systematically design assessment tasks, we apply the Evidence Centered Assessment Design (ECD, see section \ref{sec:ECD}) framework combined with validation steps by an expert panel. ECD involves an iterative process of developing, piloting, testing and evaluating an assessment design pattern. The process ensures coherence between learning objectives and tasks which elicit evidence in order to adequately assess these.  In this study, an assessment pattern (consisting of templates and example assessments) is created, implemented and deployed, and its significance and implications investigated.



This study starts with a literary review to inventory what is known about challenges that teachers face regarding assessment (as a part of PCK) in general, and pertaining to algorithms and algorithmic thinking in particular.


The study continues with a document analysis of related work pertaining to assessment of CS concepts and skills. Research on existing assessment instruments and relevant international curricula are reviewed. To select the curricula, we first listed those curricula which have been referred to in recent publications. The next selection criteria pertained to the relevancy of the learning objectives. Those must coincide with the reformed Dutch curriculum, with a particular focus on algorithms or problem solving, as well as explicitly embedding Computational Thinking as a core competency. In the third selection, relevant supportive documentation was considered, such as progression pathways (CAS, corresponding to the national curriculum in the U.K.) or specific description of the KSA outcomes and mappings to their corresponding curricula (ECS and AP CS Principles as specifications of the U.S. CSTA, and AQA as specifications of the England national curriculum). This narrowed down to the following short-list of curricula: U.S. CSTA, the Israel nationwide CS curriculum, National curriculum for computing England, and the NCEA Digital Technologies curriculum in New Zealand. From these curricula, sample assessment tasks and the way in which they elicited evidence in relation to their KSAs was analyzed together with any available research generally describing the curriculum or specifically describing the development of assessments. In addition, relevant (inquiry based) tasks from Process Oriented Guided Inquiry Learning in Computer Science (CS-POGIL), the International Baccalaureate (IB) program, and the Computer Science Field Guide \cite{CSFieldGuide} were reviewed.


Similar to the ECS, we use the structured ECD approach to establish a design template for assessments which can be fine-tuned by teachers to adhere to their own specific classroom needs. This structured approach results in a clear link between potential observations (evidence of students' knowledge or skills), evaluation procedures, and measurement models (see section \ref{sec:ECD}).\todo{evt rubrics etc}


The design pattern is piloted in two stages. The results from literary review will be triangulated with teacher interviews and observations during the implementation and delivery phases in which teachers use templates to implement and deliver their customized assessments. Via qualitative analysis (teacher interviews, student think-aloud sessions, student self-evaluation, assessment task analysis, scoring process analysis) we seek to determine which characteristics in the design-patterns are feasible as PCK-enablers, and which aspects require refinement or further investigation (see also table \ref{table:MappingCriteriaMethod}). In an iterative process, the results are analyzed and evaluated, and the assessment instrument adjusted and refined. This process is repeated twice, first in a small-scale pilot, and then the candidate template assessment tasks will be deployed at multiple institutions for testing on a larger scale. We subsequently report our findings.



\noindent\textbf{Expert panel}\newline
Intermediary results will be prone to expert review, by a carefully selected panel of experts. The group consists of (master) teachers, teacher educators and experts that have been involved in conceiving the reformed Dutch curriculum from multiple institutions (two high schools and two universities).

Different methods can be used to come to a consensus. The Delphi method is rather cost-effective. Furthermore, as the number of experts in the field can be counted on one hand, the anonymity cannot be guaranteed. Rather, similar to \citeA{catete2017framework} we choose to use a mix between the Nominal Group Technique and the Delphi technique. First, every member of the group responds to the given input, explaining their outcomes shortly. Any items for which consensus has been found is removed, in essence, creating a short-list of controversy items. The members are then again asked, now that they have heard the reasoning of their colleagues, to reconsider their view. This is repeated until a minimal consensus of 70\% is reached on each question.

The panel will contribute in the following stages:
\begin{itemize}
\item \emph{Fitness for purpose}: To ensure that the resulting KSAs do indeed match the curricular learning objectives and are complete.
\item \emph{Acceptability}: To determine if the assessment corresponds to the attitudes and views of the professional community.
\item \emph{Authenticity}: To determine if the assessment evaluates competencies needed in the future workplace.\item To validate the model assessment that created using the design pattern.\todo{how?}\todo{alignment? model corresponding ti KSAs and proper situation to elicit evidence???}
\item \emph{Fairness}: To determine if the assessment tests only relevant knowledge, skills and attitudes.
\item \emph{Cognitive Complexity}: To determine if the assessment tasks reflect the presence of higher (CT) cognitive skills and elicit the thinking process used by experts to solve complex problems.
\end{itemize}



